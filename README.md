# ğŸ¤– Rank-Bot: AI-Powered Hackathon Judge

An intelligent agent system built with the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) that automatically evaluates and ranks hackathon submissions based on concept usage, implementation difficulty, and code quality.

## ğŸ“‹ Overview

Rank-Bot was designed to judge hackathon projects from an AI Accelerator program that teaches:
- Prompt Engineering
- Multimodal AI
- RAG (Retrieval Augmented Generation)
- AI Agents (LangChain, LangGraph)

The system uses a **multi-agent architecture** with three specialized judge agents that evaluate submissions on:

1. **Concept Score (1-10)**: How many advanced concepts from the syllabus were used
2. **Difficulty Level (1-10)**: Relative implementation complexity compared to other submissions
3. **Code Quality (1-10)**: Project structure, documentation, and organization

### Key Features

- âœ… **Non-destructive Git inspection** using `git ls-tree` and `git show`
- âœ… **Handles multiple submission formats**: branches, commits, zip files
- âœ… **Calibrated scoring** using historical reference data
- âœ… **OpenRouter API** for cost-effective LLM access
- âœ… **Pydantic structured outputs** for reliable JSON parsing
- âœ… **Robust error handling** with partial run support
- âœ… **Detailed evaluation reports** in Markdown and JSON

## ğŸ—ï¸ Architecture

### Multi-Agent System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Main Orchestrator                        â”‚
â”‚                      (src/main.py)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
                â–¼             â–¼             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Concept  â”‚  â”‚Difficultyâ”‚  â”‚  Code    â”‚
        â”‚  Judge   â”‚  â”‚  Judge   â”‚  â”‚ Quality  â”‚
        â”‚  Agent   â”‚  â”‚  Agent   â”‚  â”‚  Judge   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚             â”‚             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   Tools     â”‚
                    â”‚ (Git/FS)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

| Module | Purpose |
|--------|---------|
| **`main.py`** | Orchestrates the 3-phase evaluation pipeline |
| **`agents_factory.py`** | Creates specialized judge agents with OpenRouter compatibility patches |
| **`prompts.py`** | Contains detailed instructions for each judge agent |
| **`tools.py`** | Git and filesystem tools for non-destructive code inspection |
| **`models.py`** | Pydantic models for structured LLM outputs |
| **`scoring.py`** | CSV parsing, URL parsing, report generation |
| **`config.py`** | Environment-based configuration management |

### Evaluation Pipeline

```
Phase 1: Collect Project Summaries
  â”œâ”€ Parse C4 scorecard CSV
  â”œâ”€ Extract branch/commit/path from GitHub URLs
  â””â”€ Generate summaries for difficulty calibration

Phase 2: Per-Project Scoring
  â”œâ”€ Concept Judge: Evaluate syllabus concept usage
  â””â”€ Code Quality Judge: Assess structure and docs

Phase 3: Relative Difficulty Scoring
  â””â”€ Difficulty Judge: Compare all projects and rank

Phase 4: Generate Outputs
  â”œâ”€ Update CSV with scores and rankings
  â”œâ”€ Generate detailed Markdown report
  â””â”€ Export structured JSON results
```

## ğŸš€ Setup

### Prerequisites

- **Python 3.12+**
- **uv** package manager ([installation guide](https://github.com/astral-sh/uv))
- **OpenRouter API key** ([get one here](https://openrouter.ai/))
- **Git repositories**: Cloned C3 and C4 submission repos

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd rank-bot
   ```

2. **Install dependencies**
   ```bash
   uv sync
   ```

3. **Set up environment variables**
   
   Create a `.env` file in the project root:
   ```bash
   # Required
   OPENROUTER_API_KEY=your_api_key_here
   
   # Optional (defaults shown)
   RANK_BOT_MODEL=anthropic/claude-sonnet-4
   RANK_BOT_BASE=/path/to/rank-bot  # Auto-detected if not set
   ```

4. **Verify setup**
   ```bash
   uv run python -c "from src.config import Config; print('âœ… Config loaded')"
   ```

### Directory Structure

Ensure your project has the following structure:

```
rank-bot/
â”œâ”€â”€ src/                    # Source code
â”œâ”€â”€ sheets/                 # CSV files
â”‚   â”œâ”€â”€ reference_scores.csv
â”‚   â”œâ”€â”€ current_scores.csv
â”‚   â””â”€â”€ syllabus.csv
â”œâ”€â”€ submissions_reference/  # Reference submission repo
â”œâ”€â”€ submissions_current/    # Current submission repo
â”œâ”€â”€ output/                 # Generated reports (auto-created)
â”œâ”€â”€ .env                    # Your API keys (gitignored)
â”œâ”€â”€ pyproject.toml          # Project config
â””â”€â”€ README.md
```

## ğŸ’» Usage

### Basic Evaluation

Evaluate all submissions:

```bash
uv run python src/main.py
```

### Partial Runs

Resume evaluation for specific groups (useful after API errors):

```bash
# Evaluate only groups 3, 4, and 5
uv run python src/main.py --groups 3 4 5
```

### Output Files

After running, you'll find:

- **`sheets/current_scores.csv`**: Updated with scores and rankings
- **`output/evaluation_report.md`**: Detailed Markdown report with justifications
- **`output/scores.json`**: Structured JSON data for programmatic access

### Example Output

```markdown
# Hackathon Evaluation Report

## Summary

| Rank | Group | Concept | Difficulty | Code Quality | Total |
|------|-------|---------|------------|--------------|-------|
| 1    | 12    | 10      | 10         | 9            | 29    |
| 2    | 5     | 9       | 9          | 10           | 28    |
| 3    | 8     | 10      | 10         | 8            | 28    |
...
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENROUTER_API_KEY` | **Required**. Your OpenRouter API key | - |
| `RANK_BOT_MODEL` | Model to use for evaluation | `anthropic/claude-sonnet-4` |
| `RANK_BOT_BASE` | Base directory for the project | Auto-detected |

### Model Selection

Recommended models for cost/quality balance:

- **Best quality**: `anthropic/claude-sonnet-4` (default)
- **Budget-friendly**: `google/gemini-2.0-flash-001`
- **Balanced**: `openai/gpt-4o-mini`

Change model in `.env`:
```bash
RANK_BOT_MODEL=google/gemini-2.0-flash-001
```

### Scoring Calibration

The system uses historical reference scores for calibration. Key calibration points:

- **10/10 Concept**: Multi-agent LangGraph + RAG + multimodal + advanced orchestration
- **9/10 Concept**: LangGraph + RAG + external integrations
- **8/10 Concept**: Multi-agent with RAG but fewer concept areas
- **7/10 Concept**: Multi-agent with some RAG or LangGraph
- **â‰¤6/10 Concept**: Simpler implementations or minimal concept usage

## ğŸ› ï¸ Development

### Project Structure

```
src/
â”œâ”€â”€ main.py              # Entry point and orchestration
â”œâ”€â”€ agents_factory.py    # Agent creation with SDK patches
â”œâ”€â”€ prompts.py           # Judge agent instructions
â”œâ”€â”€ tools.py             # Git/filesystem tools
â”œâ”€â”€ models.py            # Pydantic schemas
â”œâ”€â”€ scoring.py           # CSV/report utilities
â””â”€â”€ config.py            # Configuration management
```

### Key Design Patterns

1. **Functional Core, Imperative Shell**: Pure functions in `scoring.py`, side effects in `main.py`
2. **Errors as Values**: No nested try-except blocks; graceful degradation
3. **Frozen Dataclasses**: Immutable domain models (`GroupInfo`, `Config`)
4. **Pydantic for I/O**: Structured LLM outputs with validation
5. **Monkey Patches**: SDK compatibility fixes for OpenRouter

### SDK Compatibility Patches

The system includes two critical patches in `agents_factory.py`:

1. **Response Format Patch**: Forces `json_object` mode (OpenRouter doesn't support `json_schema`)
2. **JSON Extraction Patch**: Strips preamble text that Claude adds before JSON

### Adding New Tools

Tools are defined in `tools.py` using the `@function_tool` decorator:

```python
from openai_agents import function_tool

@function_tool
def my_new_tool(param: str) -> str:
    """Tool description for the LLM.
    
    Args:
        param: Parameter description
        
    Returns:
        str: Result description
    """
    # Implementation
    return result
```

Register in `tools.py`:
```python
ALL_TOOLS = [
    git_list_files,
    git_read_file,
    read_local_file,
    list_local_directory,
    extract_zip_and_list,
    my_new_tool,  # Add here
]
```

## ğŸ“Š Evaluation Criteria

### Concept Score (1-10)

Evaluates usage of accelerator concepts:

- âœ… Prompt Engineering (XML/JSON structured prompts)
- âœ… LLM APIs (OpenAI, Groq, OpenRouter)
- âœ… RAG (Vector stores, embeddings, LlamaIndex/LangChain)
- âœ… Multimodal AI (Image/audio/video processing)
- âœ… AI Agents (LangChain tools, structured output)
- âœ… Advanced Agents (LangGraph orchestration, conditional edges, loops)

**Scoring**: More concepts + sophisticated combinations = higher score

### Difficulty Level (1-10)

Relative assessment of implementation complexity:

- Graph orchestration patterns (conditional edges, loops, fan-out/fan-in)
- Multi-source RAG with parallel retrieval
- Custom vector store implementations
- External API integrations
- Hallucination detection and quality gates
- Domain-specific validation logic

**Scoring**: Comparative ranking across all submissions

### Code Quality (1-10)

Evaluates project organization:

- âœ… Proper folder structure (`agents/`, `utils/`, `tests/`, `docs/`)
- âœ… Comprehensive README with setup instructions
- âœ… `requirements.txt` or `pyproject.toml`
- âœ… `.env.example` and `.gitignore`
- âœ… No committed secrets, cache files, or binaries
- âœ… Architecture documentation

**Penalties**: ZIP submissions, committed secrets, missing docs

## ğŸ› Troubleshooting

### Common Issues

**Issue**: `OPENROUTER_API_KEY environment variable must be set`
```bash
# Solution: Create .env file with your API key
echo "OPENROUTER_API_KEY=your_key_here" > .env
```

**Issue**: `ModuleNotFoundError: No module named 'agents'`
```bash
# Solution: Install dependencies
uv sync
```

**Issue**: `APIStatusError: Error code: 402 - insufficient credits`
```bash
# Solutions:
# 1. Top up credits at https://openrouter.ai/settings/keys
# 2. Switch to a cheaper model in .env:
echo "RANK_BOT_MODEL=google/gemini-2.0-flash-001" >> .env
# 3. Resume with --groups flag to skip already-scored groups
uv run python src/main.py --groups 10 11 12
```

**Issue**: `MaxTurnsExceeded: Max turns (20) exceeded`
```bash
# This means the agent is reading too many files
# The system has efficiency guidance built-in, but you can:
# 1. Increase max_turns in main.py (already set to 20)
# 2. Check if the project has excessive files
```

### Debug Mode

Enable detailed logging:

```bash
# In your .env
LOG_LEVEL=DEBUG

# Or inline
LOG_LEVEL=DEBUG uv run python src/main.py
```

## ğŸ“ License

This project is for educational and evaluation purposes as part of the AI Accelerator program.

## ğŸ™ Acknowledgments

- Built with [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)
- Powered by [OpenRouter](https://openrouter.ai/) for multi-model access
- Uses [uv](https://github.com/astral-sh/uv) for fast Python package management

---

**Made with â¤ï¸ for the AI Accelerator Hackathon**
